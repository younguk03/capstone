from datetime import datetime
import os
import pandas as pd
import requests
import time
import textwrap
from fpdf.enums import XPos, YPos
from urllib.parse import urlparse, urljoin, parse_qs, urlencode
from bs4 import BeautifulSoup
from fpdf import FPDF
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import NoAlertPresentException
from concurrent.futures import ThreadPoolExecutor

xss_payloads = [
    "<script>alert(1)</script>", "'><svg/onload=confirm(1)>",
    "<img src=x onerror=alert(1)>", "<svg><script>confirm(1)</script>",
    "<body onload=prompt(1)>"
]

def safe_multiline(pdf, text, width=90):
    for line in textwrap.wrap(text, width=width):
        pdf.cell(0, 8, line, new_x=XPos.LMARGIN, new_y=YPos.NEXT)

def fetch_links(url, base_url, visited):
    found_links = []
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=3)
        soup = BeautifulSoup(res.text, "html.parser")
        visited.add(url)
        if "?" in url and "=" in url:
            found_links.append(('get', url))
        if soup.find("form"):
            found_links.append(('form', url))
        for a in soup.find_all("a", href=True):
            link = urljoin(url, a['href'])
            if urlparse(link).netloc == urlparse(base_url).netloc and link not in visited:
                found_links.append(('next', link))
    except:
        pass
    return found_links

def smart_crawl_site(base_url, max_links=10):
    visited, to_visit = set(), [base_url]
    get_targets, form_targets = [], []
    with ThreadPoolExecutor(max_workers=10) as executor:
        while to_visit and len(visited) < max_links:
            futures = [executor.submit(fetch_links, url, base_url, visited) for url in to_visit[:10]]
            to_visit = to_visit[10:]
            for future in futures:
                for kind, link in future.result():
                    if kind == 'get': get_targets.append(link)
                    elif kind == 'form': form_targets.append(link)
                    elif kind == 'next' and link not in visited: to_visit.append(link)
    no_param = [u for u in visited if "?" not in u]
    fake_param_urls = [u + "?test=x" for u in no_param]
    return list(set(get_targets + fake_param_urls + form_targets))

def run_scan(url):
    checked_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    security_headers = {
        "Content-Security-Policy": ["default-src 'self'"],
        "X-Frame-Options": ["DENY", "SAMEORIGIN"],
        "Strict-Transport-Security": ["max-age=63072000; includeSubDomains"],
        "X-Content-Type-Options": ["nosniff"],
        "Referrer-Policy": ["no-referrer", "strict-origin-when-cross-origin"]
    }
    results = []
    try:
        response = requests.get(url, timeout=5)
    except Exception as e:
        return {"url": url, "checked_at": checked_at, "error": str(e), "results": []}
    for header, expected_values in security_headers.items():
        actual_value = response.headers.get(header)
        if actual_value is None:
            status = "missing"
        elif actual_value not in expected_values:
            status = "warning"
        else:
            status = "ok"
        results.append({"header": header, "status": status, "actual": actual_value, "recommended": expected_values})
    return {"url": url, "checked_at": checked_at, "results": results}

def detect_get_param_xss(url):
    results = []
    parsed = urlparse(url)
    query = parse_qs(parsed.query)
    if not query:
        return []
    for payload in xss_payloads:
        new_query = {key: payload for key in query}
        new_url = parsed._replace(query=urlencode(new_query, doseq=True)).geturl()
        try:
            r = requests.get(new_url, timeout=5)
            if payload in r.text:
                results.append((url, new_url, "GET", payload))
        except:
            continue
    return results

def detect_form_xss(url):
    results = []
    try:
        res = requests.get(url, timeout=5)
        soup = BeautifulSoup(res.text, "html.parser")
        forms = soup.find_all("form")
        for form in forms:
            action = form.get("action")
            method = form.get("method", "get").lower()
            inputs = form.find_all(["input", "textarea"])
            form_url = urljoin(url, action)
            for payload in xss_payloads:
                data = {tag.get("name"): payload for tag in inputs if tag.get("name")}
                try:
                    if method == "post":
                        r = requests.post(form_url, data=data, timeout=5)
                    else:
                        r = requests.get(form_url, params=data, timeout=5)
                    if payload in r.text:
                        results.append((url, form_url, method.upper(), payload))
                        break
                except:
                    continue
    except:
        pass
    return results

def detect_sqli(url):
    sqli_payloads = ["'", '"', "'--", '"--', "')--", "' or '1'='1", '" or "1"="1"', "' OR sleep(5)--"]
    sqli_errors = ["sql syntax", "mysql_fetch", "ORA-00933"]
    parsed = urlparse(url)
    query = parse_qs(parsed.query)
    if not query:
        return []
    results = []
    for param in query:
        for payload in sqli_payloads:
            mod_query = query.copy()
            mod_query[param] = payload
            injected_url = parsed._replace(query=urlencode(mod_query, doseq=True)).geturl()
            try:
                start = time.time()
                r = requests.get(injected_url, timeout=8)
                delay = time.time() - start
                if any(err in r.text.lower() for err in sqli_errors) or delay > 4.5:
                    results.append((url, injected_url, "SQLi", payload))
                    break
            except:
                continue
    return results

def detect_csrf(url):
    results = []
    try:
        res = requests.get(url, timeout=5)
        soup = BeautifulSoup(res.text, "html.parser")
        cookies = res.headers.get("Set-Cookie", "").lower()
        for form in soup.find_all("form"):
            if form.get("method", "get").lower() == "post":
                names = [i.get("name", "").lower() for i in form.find_all("input")]
                if not any("csrf" in n or "token" in n for n in names):
                    notes = ["CSRF í† í° ì—†ìŒ"]
                    if "samesite" not in cookies: notes.append("SameSite ì¿ í‚¤ ì—†ìŒ")
                    if "secure" not in cookies: notes.append("Secure ì¿ í‚¤ ì—†ìŒ")
                    results.append((url, urljoin(url, form.get("action")), "CSRF", " | ".join(notes)))
    except:
        pass
    return results

def detect_dom_xss_selenium(urls):
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--window-size=1280,800")
    results = []
    try:
        driver = webdriver.Chrome(service=Service("chromedriver.exe"), options=chrome_options)
        for url in urls:
            try:
                driver.get(url)
                time.sleep(1)
                for payload in xss_payloads:
                    for elem in driver.find_elements(By.TAG_NAME, "input"):
                        try: elem.clear(); elem.send_keys(payload)
                        except: continue
                    for b in driver.find_elements(By.TAG_NAME, "button"):
                        try: b.click()
                        except: continue
                    time.sleep(1)
                    try:
                        alert = driver.switch_to.alert
                        alert.accept()
                        results.append((url, url, "DOM", payload))
                        break
                    except NoAlertPresentException:
                        continue
            except: continue
        driver.quit()
    except Exception as e:
        print(f"[\u274c \ub4dc\ub77c\uc774\ubc84 \uc2e4\ud589 \uc2e4\ud328] {e}")
    return results

def generate_korean_pdf_report(results, header_reports, target_url):
    pdf = FPDF()
    base_dir = os.getcwd()
    font_path = os.path.join(base_dir, "NanumGothic-Regular.ttf")
    bold_path = os.path.join(base_dir, "NanumGothic-Bold.ttf")

    pdf.add_font("Nanum", "", font_path)
    pdf.add_font("Nanum", "B", bold_path)
    pdf.set_font("Nanum", size=12)
    pdf.add_page()

    safe_multiline(pdf, "ì›¹ í†µí•© ì·¨ì•½ì  íƒì§€ ë³´ê³ ì„œ")
    safe_multiline(pdf, f"ëŒ€ìƒ ì‚¬ì´íŠ¸: {target_url}")
    safe_multiline(pdf, f"ì´ íƒì§€ëœ ì·¨ì•½ì  ìˆ˜: {len(results)}ê±´")
    pdf.ln(5)

    for i, (origin, tested_url, method, payload) in enumerate(results, 1):
        pdf.set_font("Nanum", size=12)
        safe_multiline(pdf, f"{i}. ì›ë³¸: {origin}")
        safe_multiline(pdf, f"ë°©ì‹: {method}")
        safe_multiline(pdf, f"í˜ì´ë¡œë“œ: {payload}")
        safe_multiline(pdf, f"URL: {tested_url}")
        pdf.ln(3)

    pdf.add_page()
    pdf.set_font("Nanum", size=12)
    safe_multiline(pdf, "[ë³´ì•ˆ í—¤ë” ì ê²€ ê²°ê³¼]")

    for report in header_reports:
        pdf.set_font("Nanum", "B", size=12)
        safe_multiline(pdf, f"URL: {report['url']}")
        pdf.set_font("Nanum", size=11)
        if "error" in report:
            safe_multiline(pdf, f"ì—ëŸ¬: {report['error']}")
        else:
            for res in report.get("results", []):
                header = res.get("header", "N/A")
                status = res.get("status", "N/A")
                actual = res.get("actual") or "ì—†ìŒ"
                line = f"â€¢ {header} : ìƒíƒœ = {status} / ì‹¤ì œ ê°’ = {actual}"
                safe_multiline(pdf, line)
        pdf.ln(3)

    report_dir = os.path.join(base_dir, "static", "reports")
    os.makedirs(report_dir, exist_ok=True)
    filename = f"full_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
    filepath = os.path.join(report_dir, filename)

    try:
        pdf.output(filepath)
        print(f"[ğŸ“„ PDF ì €ì¥ ì„±ê³µ] {filepath}")
    except Exception as e:
        print(f"[âŒ PDF ì €ì¥ ì‹¤íŒ¨] {e}")
        filepath = None

    return filepath

def run_full_scan(base_url):
    pages = smart_crawl_site(base_url)
    all_results = []
    header_reports = []
    print(f"\n[ğŸ” ì´ {len(pages)}ê°œ í•˜ìœ„ URL íƒì§€ë¨]")
    for i, page in enumerate(pages, 1):
        print(f"\n[{i}/{len(pages)}] ëŒ€ìƒ: {page}")
        header_reports.append(run_scan(page))
        all_results.extend(detect_form_xss(page))
        if '?' in page:
            all_results.extend(detect_get_param_xss(page))
            all_results.extend(detect_sqli(page))
        all_results.extend(detect_csrf(page))
    all_results.extend(detect_dom_xss_selenium(pages))
    print("\n[âœ… ì „ì²´ ì ê²€ ì™„ë£Œ]")
    if all_results:
        df = pd.DataFrame(all_results, columns=["ì›ë³¸", "íƒì§€ URL", "ê¸°ë²•", "í˜ì´ë¡œë“œ"])
        print(df)
    else:
        print("íƒì§€ëœ ì·¨ì•½ì ì´ ì—†ìŠµë‹ˆë‹¤.")
    pdf_path = generate_korean_pdf_report(all_results, header_reports, base_url)
    print(f"[ğŸ“„ PDF ë³´ê³ ì„œ ì €ì¥ ìœ„ì¹˜] {pdf_path}")

if __name__ == "__main__":
    url = input("ë¶„ì„í•  URL ì…ë ¥: ")
    run_full_scan(url)
